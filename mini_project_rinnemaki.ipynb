{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b656d6aa-5b18-492c-bf16-2e85ddfd483a",
   "metadata": {},
   "source": [
    "# Mini-Project: Cyber Czech 2019 Exercise Traffic and Log Analysis\n",
    "# Writers name \n",
    "## date 22/04/2025\n",
    "## Individual project\n",
    "This notebook presents a story-driven analysis of the network traffic flows and event logs captured during the Cyber Czech 2019 Red Team/Blue Team exercise at Masaryk University, Brno, Czech Republic. We will:\n",
    "\n",
    "1. Introduce the dataset and its context.\n",
    "\n",
    "2. Load and clean each data source (IPFIX flows, Syslog, Winlog).\n",
    "\n",
    "3. Tag events related to Red Team activity.\n",
    "\n",
    "4. Explore and visualize trends and patterns over time.\n",
    "\n",
    "5. Interpret findings and suggest next steps for deeper analysis.\n",
    "\n",
    "## Dataset Overview\n",
    "The data was downloaded from the following link: https://zenodo.org/record/3746129\n",
    "\n",
    "The data were collected over two distinct intervals:\n",
    "\n",
    "- Day 1: March 19, 2019, 11:00 – 18:00 (CET)\n",
    "\n",
    "- Day 2: March 20, 2019, 08:00 – 15:30 (CET)\n",
    "\n",
    "Three event types are provided in JSON format:\n",
    "\n",
    "- cz.muni.csirt.IpfixEntry: IPFIX flows with parsed application protocols.\n",
    "\n",
    "- cz.muni.csirt.SyslogEntry: Linux Syslog entries.\n",
    "\n",
    "- cz.muni.csirt.WinlogEntry: Windows Event Log entries.\n",
    "\n",
    "Auxiliary files include network topology and Red Team IP/hostname schedules for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d607486-b91f-4485-b4aa-de64d5f33a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & global config\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import json\n",
    "import ipaddress\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define paths\n",
    "NB_ROOT   = pathlib.Path(\".\").resolve()\n",
    "DATA_ROOT = NB_ROOT\n",
    "\n",
    "IPFIX_GZ   = DATA_ROOT / \"cz.muni.csirt.IPFlowEntry\" / \"data.json.gz\"\n",
    "SYSLOG_GZ  = DATA_ROOT / \"cz.muni.csirt.SyslogEntry\" / \"data.json.gz\"\n",
    "WINLOG_GZ  = DATA_ROOT / \"cz.muni.csirt.WinlogEntry\" / \"data.json.gz\"\n",
    "AUX_DIR    = DATA_ROOT / \"auxiliary-material\"\n",
    "\n",
    "RED_IP_CSV = AUX_DIR / \"redteam-reserved-ip-ranges.csv\"\n",
    "ATTACK_CSV = AUX_DIR / \"redteam-attack-schedule.csv\"\n",
    "\n",
    "PRG_TZ = \"Europe/Prague\"\n",
    "\n",
    "# Configure display and plotting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc6f34-87da-4955-92d4-c6891a9cac67",
   "metadata": {},
   "source": [
    "Explanation: We import libraries for data manipulation (pandas, numpy), visualization (matplotlib, seaborn), file I/O (gzip, json), IP range handling (ipaddress), and operating-system paths (pathlib). We define constants pointing to our data files and auxiliary CSVs using the same file structure as the original notebook. We also set the Prague timezone for consistency and configure display and plotting styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29d21520-42c8-4f13-acb8-06b985f9a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities\n",
    "def load_json_gz(path):\n",
    "    return pd.read_json(path, lines=True, compression=\"gzip\")\n",
    "\n",
    "\n",
    "def stream_json_gz(path, chunksize=100_000):\n",
    "    \"\"\"Stream large .json.gz line-by-line in chunks.\"\"\"\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        rows = []\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "            if len(rows) >= chunksize:\n",
    "                yield pd.DataFrame(rows)\n",
    "                rows.clear()\n",
    "        if rows:\n",
    "            yield pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def to_prague(series):\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\")\n",
    "    if dt.dt.tz is None:\n",
    "        return dt.dt.tz_localize(PRG_TZ, nonexistent=\"shift_forward\")\n",
    "    else:\n",
    "        return dt.dt.tz_convert(PRG_TZ)\n",
    "\n",
    "\n",
    "def cidr_dataframe(csv_path):\n",
    "    txt = csv_path.read_text().strip()\n",
    "    cidrs = [c.strip() for c in txt.split(\",\") if c.strip()]\n",
    "    nets  = [ipaddress.ip_network(c) for c in cidrs]\n",
    "    return pd.DataFrame({\"network\": nets})\n",
    "\n",
    "def preview(df, n=5):\n",
    "    \"\"\"Display the first n rows of a DataFrame with context.\"\"\"\n",
    "    print(f\"Previewing first {n} rows of DataFrame ({len(df)} total rows):\")\n",
    "    display(df.head(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de789f-ae54-41e0-8cd2-3da8c266ad89",
   "metadata": {},
   "source": [
    "Explanation: Helper functions : load_json_gz for reading JSON lines with gzip compression, stream_json_gz for chunked streaming, to_prague for timezone localization/conversion, and cidr_dataframe for loading CIDR ranges into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "279c50c6-2c66-4e20-8e5c-0d092d365bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing first 5 rows of DataFrame (28 total rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.3.0.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212.5.0.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>213.5.0.0/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202.2.96.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.5.80.0/20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ip_range\n",
       "0    27.3.0.0/19\n",
       "1   212.5.0.0/19\n",
       "2   213.5.0.0/21\n",
       "3  202.2.96.0/19\n",
       "4  110.5.80.0/20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known Red-Team hostnames: ['redteam-win01', 'redteam-linux01', 'redteam-gateway01']\n"
     ]
    }
   ],
   "source": [
    "# Load and expand Red-Team IP ranges\n",
    "df_red_ips = cidr_dataframe(RED_IP_CSV)\n",
    "preview(df_red_ips.rename(columns={'network':'ip_range'}))\n",
    "\n",
    "# Define known Red-Team hostnames manually (from topology or exercise documentation)\n",
    "red_hosts = [\n",
    "    # Example hostnames -- replace with actual names extracted from the topology\n",
    "    'redteam-win01',\n",
    "    'redteam-linux01',\n",
    "    'redteam-gateway01',\n",
    "]\n",
    "print(\"Known Red-Team hostnames:\", red_hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64f3105a-879c-4528-9099-bd15d5f65ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28 Red-Team IP ranges\n",
      "\n",
      "Red-Team Networks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.3.0.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212.5.0.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>213.5.0.0/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202.2.96.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.5.80.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200.110.240.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.231.64.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>181.118.144.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37.32.0.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>193.151.128.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.111.240.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>80.93.176.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>130.255.32.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>212.96.96.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>217.25.208.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.23.128.0/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>219.15.224.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.172.192.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>80.79.0.0/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>92.53.192.0/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>78.177.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.9.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>111.66.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>188.40.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>129.90.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>77.51.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37.6.0.0/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>81.17.0.0/20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             network\n",
       "0        27.3.0.0/19\n",
       "1       212.5.0.0/19\n",
       "2       213.5.0.0/21\n",
       "3      202.2.96.0/19\n",
       "4      110.5.80.0/20\n",
       "5   200.110.240.0/20\n",
       "6     66.231.64.0/20\n",
       "7   181.118.144.0/20\n",
       "8       37.32.0.0/19\n",
       "9   193.151.128.0/19\n",
       "10   27.111.240.0/20\n",
       "11    80.93.176.0/20\n",
       "12   130.255.32.0/19\n",
       "13    212.96.96.0/19\n",
       "14   217.25.208.0/20\n",
       "15     5.23.128.0/17\n",
       "16   219.15.224.0/20\n",
       "17    5.172.192.0/20\n",
       "18      80.79.0.0/20\n",
       "19    92.53.192.0/19\n",
       "20     78.177.0.0/16\n",
       "21        1.9.0.0/16\n",
       "22     111.66.0.0/16\n",
       "23     188.40.0.0/16\n",
       "24     129.90.0.0/16\n",
       "25      77.51.0.0/16\n",
       "26       37.6.0.0/16\n",
       "27      81.17.0.0/20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw attack schedule loaded\n",
      "\n",
      "Attack Schedule Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exercise time[hh:mm]</th>\n",
       "      <th>Attack type</th>\n",
       "      <th>Importance</th>\n",
       "      <th>Affected network segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00 – 00:30</td>\n",
       "      <td>Network reconnaissance</td>\n",
       "      <td>Low</td>\n",
       "      <td>DMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:30 – 01:00</td>\n",
       "      <td>Denial of service</td>\n",
       "      <td>Low</td>\n",
       "      <td>DMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01:00 – 02:00</td>\n",
       "      <td>Web attacks</td>\n",
       "      <td>Medium</td>\n",
       "      <td>DMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01:30 – 02:00</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>Medium</td>\n",
       "      <td>DMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02:00 – 02:40</td>\n",
       "      <td>Web attacks</td>\n",
       "      <td>High</td>\n",
       "      <td>DMZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Exercise time[hh:mm]             Attack type Importance Affected network segments\n",
       "0        00:00 – 00:30  Network reconnaissance        Low                       DMZ\n",
       "1        00:30 – 01:00       Denial of service        Low                       DMZ\n",
       "2        01:00 – 02:00             Web attacks     Medium                       DMZ\n",
       "3        01:30 – 02:00                Phishing     Medium                       DMZ\n",
       "4        02:00 – 02:40             Web attacks       High                       DMZ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load & prepare Red-Team data \n",
    "red_ranges = cidr_dataframe(RED_IP_CSV)\n",
    "print(f\"Loaded {len(red_ranges)} Red-Team IP ranges\")\n",
    "print(\"\\nRed-Team Networks:\")\n",
    "display(red_ranges)\n",
    "\n",
    "# Load attack schedule\n",
    "attack_raw = pd.read_csv(ATTACK_CSV, comment=\"#\")\n",
    "print(\"Raw attack schedule loaded\")\n",
    "\n",
    "# Normalize column names and preview\n",
    "attack_raw.columns = attack_raw.columns.str.strip()\n",
    "print(\"\\nAttack Schedule Preview:\")\n",
    "display(attack_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cec8ee-d54d-431e-9887-28b7d45560f2",
   "metadata": {},
   "source": [
    "Explanation: We load the CSV of reserved IP ranges into a DataFrame of ipaddress.ip_network objects. Since the attack schedule CSV does not include hostnames, we manually define the list of Red-Team hostnames based on the network topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86104cc9-6ad9-4bee-baf9-8f2a14685a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean IPFIX flow data \n",
    "df_ipfix = load_json_gz(IPFIX_GZ)\n",
    "print(\"IPFIX raw rows:\", len(df_ipfix))\n",
    "\n",
    "# Handle duplicate 'timestamp' columns if present\n",
    "duplicate_columns = df_ipfix.columns[df_ipfix.columns.duplicated()].tolist()\n",
    "if duplicate_columns:\n",
    "    print(\"Dropping duplicate columns:\", duplicate_columns)\n",
    "    df_ipfix = df_ipfix.loc[:, ~df_ipfix.columns.duplicated()]\n",
    "\n",
    "# Convert time\n",
    "if \"flowStartMilliseconds\" in df_ipfix.columns:\n",
    "    df_ipfix[\"time\"] = pd.to_datetime(df_ipfix[\"flowStartMilliseconds\"], unit=\"ms\", errors=\"coerce\")\n",
    "    df_ipfix[\"time\"] = to_prague(df_ipfix[\"time\"])\n",
    "else:\n",
    "    raise ValueError(\"Missing expected time column in IPFIX flows\")\n",
    "\n",
    "# Drop nulls and sort\n",
    "df_ipfix_clean = (\n",
    "    df_ipfix.dropna(subset=[\"time\", \"sourceIPv4Address\", \"destinationIPv4Address\"])\n",
    "            .sort_values(\"time\", ignore_index=True)\n",
    ")\n",
    "\n",
    "print(\"Cleaned IPFIX rows:\", len(df_ipfix_clean))\n",
    "print(\"IPFIX Time Span:\", df_ipfix_clean.time.min(), \"→\", df_ipfix_clean.time.max())\n",
    "\n",
    "# Preview cleaned data\n",
    "preview(df_ipfix_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f9f25-d453-4df0-bd88-28e6d60fd5d5",
   "metadata": {},
   "source": [
    "Explanation: We directly load the entire IPFIX dataset using load_json_gz, handle any duplicate columns, convert the flowStartMilliseconds to a timezone-aware time column localized to Prague, and drop rows missing key fields. We then sort by time and preview the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d184c9-e7df-48ee-88ea-e21a2bbd827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean Syslog data \n",
    "# Large dataset – streamed in chunks to avoid memory errors\n",
    "syslog_chunks = []\n",
    "for chunk in stream_json_gz(SYSLOG_GZ):\n",
    "    if \"timegenerated\" in chunk.columns:\n",
    "        chunk[\"time\"] = to_prague(chunk[\"timegenerated\"])\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'timegenerated' column in Syslog dataset\")\n",
    "\n",
    "    chunk = chunk.dropna(subset=[\"time\"])\n",
    "    syslog_chunks.append(chunk)\n",
    "\n",
    "df_syslog_clean = pd.concat(syslog_chunks, ignore_index=True).sort_values(\"time\", ignore_index=True)\n",
    "\n",
    "print(\"Cleaned Syslog rows:\", len(df_syslog_clean))\n",
    "print(\"Syslog Time Span:\", df_syslog_clean.time.min(), \"→\", df_syslog_clean.time.max())\n",
    "\n",
    "# Preview cleaned Syslog DataFrame\n",
    "preview(df_syslog_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f728f8f-5ad8-4b41-a543-52b5b4c9bb26",
   "metadata": {},
   "source": [
    "Explanation: We stream Syslog records in chunks, converting the timegenerated field to a timezone-aware time column localized to Prague. We then drop rows missing the timestamp, concatenate all chunks, sort by time, and preview the cleaned dataset. This chunked approach ensures memory efficiency for large logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9fbeb-3aaa-4b9a-b811-1b0c4a722e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean Winlog data \n",
    "# Large dataset – streamed in chunks to avoid memory errors\n",
    "winlog_chunks = []\n",
    "for chunk in stream_json_gz(WINLOG_GZ):\n",
    "    if \"orig_timestamp\" in chunk.columns:\n",
    "        chunk[\"time\"] = to_prague(chunk[\"orig_timestamp\"])\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'orig_timestamp' column in Winlog dataset\")\n",
    "\n",
    "    chunk = chunk.dropna(subset=[\"time\"])\n",
    "    winlog_chunks.append(chunk)\n",
    "\n",
    "df_winlog_clean = pd.concat(winlog_chunks, ignore_index=True).sort_values(\"time\", ignore_index=True)\n",
    "\n",
    "print(\"Cleaned Winlog rows:\", len(df_winlog_clean))\n",
    "print(\"Winlog Time Span:\", df_winlog_clean.time.min(), \"→\", df_winlog_clean.time.max())\n",
    "\n",
    "print(df_winlog_clean.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541bd10-2d26-4cf1-957e-04db9f8c4455",
   "metadata": {},
   "source": [
    "Explanation: We iterate through the Winlog JSON lines in chunks, converting the timeCreated field to a timezone-aware time column localized to Prague. We filter out any entries missing required fields (time, xmlPayload), then concatenate and sort all chunks into a single DataFrame. This ensures efficient processing and yields a clean Winlog dataset ready for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11d1d8-3fbd-4265-8e69-61eea81a275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tags on cleaned Winlog DataFrame\n",
    "df_winlog_clean['ip_red'] = df_winlog_clean['xml'].apply(\n",
    "    lambda text: any(str(net.network_address) in text for net in df_red_ips['network'])\n",
    ")\n",
    "df_winlog_clean['host_red'] = df_winlog_clean['xml'].apply(\n",
    "    lambda text: any(host in text for host in red_hosts)\n",
    ")\n",
    "preview(df_winlog_clean[['time','xml','ip_red','host_red']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b315a-f9a0-4f3a-aa0e-892da7cd5270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da28467-4f0b-4ed4-87ec-3fafec3af37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tag IPFIX flows involving Red-Team IPs \n",
    "def is_red_ip(ip_str):\n",
    "    ip = ipaddress.ip_address(ip_str)\n",
    "    return any(ip in net for net in red_ranges[\"network\"])\n",
    "\n",
    "df_ipfix_clean[\"red_source\"] = df_ipfix_clean[\"sourceIPv4Address\"].apply(is_red_ip)\n",
    "df_ipfix_clean[\"red_dest\"]   = df_ipfix_clean[\"destinationIPv4Address\"].apply(is_red_ip)\n",
    "df_ipfix_clean[\"involves_red\"] = df_ipfix_clean[\"red_source\"] | df_ipfix_clean[\"red_dest\"]\n",
    "\n",
    "print(\"Tagged IPFIX flows involving Red-Team IPs:\", df_ipfix_clean[\"involves_red\"].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1599b3-4f85-43b0-a769-480af201a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Syslog entries mentioning Red-Team IPs \n",
    "import re\n",
    "\n",
    "# Precompile regex patterns for all red-team IPs\n",
    "ip_patterns = [re.escape(str(net)) for net in red_ranges[\"network\"]]\n",
    "regex_pattern = re.compile(r\"|\".join(ip_patterns))\n",
    "\n",
    "# Flag Syslog messages containing any red-team IP\n",
    "df_syslog_clean[\"mentions_red_ip\"] = df_syslog_clean[\"message\"].astype(str).apply(lambda msg: bool(regex_pattern.search(msg)))\n",
    "\n",
    "print(\"Tagged Syslog entries mentioning Red-Team IPs:\", df_syslog_clean[\"mentions_red_ip\"].sum())\n",
    "print(\"Sample tagged Syslog rows:\")\n",
    "display(df_syslog_clean[df_syslog_clean[\"mentions_red_ip\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63df65a-3789-461d-bf4d-93f3ac47de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Syslog entries mentioning Red-Team IPs \n",
    "import re\n",
    "\n",
    "# Expand all red-team CIDRs into IP address strings\n",
    "red_ips = set()\n",
    "for net in red_ranges[\"network\"]:\n",
    "    # limit expansion for very large ranges\n",
    "    if net.num_addresses <= 1024:\n",
    "        red_ips.update(str(ip) for ip in net.hosts())\n",
    "\n",
    "# Compile a regex pattern for exact IP address matches\n",
    "ip_regex_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, red_ips)) + r\")\\b\")\n",
    "\n",
    "# Tag messages mentioning Red-Team IPs\n",
    "df_syslog_clean[\"mentions_red_ip\"] = df_syslog_clean[\"message\"].astype(str).apply(\n",
    "    lambda msg: bool(ip_regex_pattern.search(msg))\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(\"Tagged Syslog entries mentioning Red-Team IPs:\", df_syslog_clean[\"mentions_red_ip\"].sum())\n",
    "print(\"Sample tagged Syslog rows:\")\n",
    "display(df_syslog_clean[df_syslog_clean[\"mentions_red_ip\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a8e7f-b8db-4cad-a7af-ab9895ece4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract & count Red-Team IPs in Syslog messages \n",
    "# A more thorough scan of message fields for Red-Team IPs \n",
    "ip_regex = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "\n",
    "matched_red_ips = set()\n",
    "redteam_count = 0\n",
    "\n",
    "for msg in df_syslog_clean[\"message\"]:\n",
    "    found_ips = ip_regex.findall(str(msg))\n",
    "    for ip in found_ips:\n",
    "        if is_red_ip(ip):\n",
    "            matched_red_ips.add(ip)\n",
    "            redteam_count += 1\n",
    "\n",
    "print(f\"Red Team IPs found in message field: {redteam_count}\")\n",
    "print(\"Unique Red Team IPs matched:\")\n",
    "print(matched_red_ips)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79105b-2131-4a6c-ad14-01075b33447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Winlog entries mentioning Red-Team IPs \n",
    "# Check all string fields that could hold IPs (e.g., 'message' or 'xml')\n",
    "winlog_text_field = \"xml\" if \"xml\" in df_winlog_clean.columns else \"message\"\n",
    "\n",
    "df_winlog_clean[\"mentions_red_ip\"] = df_winlog_clean[winlog_text_field].astype(str).apply(lambda msg: bool(regex_pattern.search(msg)))\n",
    "\n",
    "print(\"Tagged Winlog entries mentioning Red-Team IPs:\", df_winlog_clean[\"mentions_red_ip\"].sum())\n",
    "print(\" Sample tagged Winlog rows:\")\n",
    "display(df_winlog_clean[df_winlog_clean[\"mentions_red_ip\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0aefcc-2cdb-4850-9799-31daa1bb453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red-Team IPs in IPFIX \n",
    "df_red_in_ipfix = df_ipfix_clean[\n",
    "    df_ipfix_clean[\"sourceIPv4Address\"].isin(matched_red_ips) |\n",
    "    df_ipfix_clean[\"destinationIPv4Address\"].isin(matched_red_ips)\n",
    "]\n",
    "\n",
    "print(\"IPFIX rows with Red-Team IPs as src or dst:\", len(df_red_in_ipfix))\n",
    "display(df_red_in_ipfix.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffede4-12d9-4125-97fc-ce0df2a1f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red-Team IPs in Winlog entries \n",
    "ip_regex = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")  # in case not already defined\n",
    "\n",
    "df_winlog_clean[\"mentions_red_ip\"] = df_winlog_clean[\"xml\"].astype(str).apply(\n",
    "    lambda text: any(ip in matched_red_ips for ip in ip_regex.findall(text))\n",
    ")\n",
    "\n",
    "print(\"Winlog entries mentioning Red-Team IPs:\", df_winlog_clean[\"mentions_red_ip\"].sum())\n",
    "display(df_winlog_clean[df_winlog_clean[\"mentions_red_ip\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c9afd-f793-4530-8a53-c60236166cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syslog messages exactly matching known Red-Team IPs\n",
    "df_syslog_matched_ips = df_syslog_clean[df_syslog_clean[\"message\"].astype(str).apply(\n",
    "    lambda msg: any(ip in msg for ip in matched_red_ips)\n",
    ")]\n",
    "\n",
    "print(\"Syslog rows containing known Red-Team IPs:\", len(df_syslog_matched_ips))\n",
    "display(df_syslog_matched_ips.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fc491-1d5e-4e96-be1c-1f8c72e41a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary counts for Red-Team activity \n",
    "print(\"\\nSummary of Red-Team tagged entries across datasets:\")\n",
    "print(\"IPFIX flows involving Red-Team IPs:\", df_ipfix_clean[\"involves_red\"].sum())\n",
    "print(\"Syslog entries mentioning Red-Team IPs:\", df_syslog_clean[\"mentions_red_ip\"].sum())\n",
    "print(\"Winlog entries mentioning Red-Team IPs:\", df_winlog_clean[\"mentions_red_ip\"].sum())\n",
    "print(\"Unique Red-Team IPs found in logs:\", len(matched_red_ips))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf6b54-8f5e-46d6-a9f0-f3b9ad408198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization \n",
    "# Create hourly time bins for each log type tagged with Red-Team activity\n",
    "ipfix_hourly = df_ipfix_clean[df_ipfix_clean[\"involves_red\"]].set_index(\"time\").resample(\"1h\").size()\n",
    "syslog_hourly = df_syslog_clean[df_syslog_clean[\"mentions_red_ip\"]].set_index(\"time\").resample(\"1h\").size()\n",
    "winlog_hourly = df_winlog_clean[df_winlog_clean[\"mentions_red_ip\"]].set_index(\"time\").resample(\"1h\").size()\n",
    "\n",
    "# Store for later visualization\n",
    "activity_by_hour = pd.DataFrame({\n",
    "    \"IPFIX\": ipfix_hourly,\n",
    "    \"Syslog\": syslog_hourly,\n",
    "    \"Winlog\": winlog_hourly\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "print(\"Hourly Red-Team activity prepared for plotting\")\n",
    "display(activity_by_hour.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d38ea-bcef-4544-a1c5-85a568b5daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of Red-Team activity over time \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "activity_by_hour.plot(ax=plt.gca(), marker=\"o\")\n",
    "\n",
    "plt.title(\"Red-Team Activity Over Time (Hourly Resolution)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Tagged Event Count\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149aa275-6cf6-420e-b05f-5a061760a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red-Team Syslog events by severity \n",
    "if \"severity\" in df_syslog_clean.columns:\n",
    "    severity_summary = (\n",
    "        df_syslog_clean[df_syslog_clean[\"mentions_red_ip\"]]\n",
    "        .groupby(\"severity\")[\"message\"]\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(\"Syslog Red-Team Events by Severity:\")\n",
    "    display(severity_summary)\n",
    "else:\n",
    "    print(\"No 'severity' column found in Syslog dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bd3ab-c51d-4621-880f-b464918da88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syslog Severity Levels Reference \n",
    "severity_levels = {\n",
    "    0: \"Emergency — system is unusable\",\n",
    "    1: \"Alert — action must be taken immediately\",\n",
    "    2: \"Critical — critical conditions\",\n",
    "    3: \"Error — error conditions\",\n",
    "    4: \"Warning — warning conditions\",\n",
    "    5: \"Notice — normal but significant\",\n",
    "    6: \"Informational — informational messages\",\n",
    "    7: \"Debug — debug-level messages\"\n",
    "}\n",
    "\n",
    "print(\"Syslog Severity Level Definitions (Defined by RFC 5424 / RFC 3164):\")\n",
    "for level, meaning in severity_levels.items():\n",
    "    print(f\"  {level}: {meaning}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64577c-f9a4-40cc-9282-18cce4095b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of Syslog Red-Team severity levels\n",
    "if 'severity_summary' in globals():\n",
    "    severity_summary.plot(kind=\"bar\", figsize=(10, 4), color=\"tomato\")\n",
    "    plt.title(\"Syslog Red-Team Events by Severity Level\")\n",
    "    plt.xlabel(\"Severity Level\")\n",
    "    plt.ylabel(\"Message Count\")\n",
    "    plt.grid(True, axis=\"y\", linestyle=\":\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No severity summary available. Run previous severity summary cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a864da-cd1e-423b-a0ad-e03886d25941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syslog Red-Team severity over time \n",
    "if \"severity\" in df_syslog_clean.columns:\n",
    "    severity_timeline = (\n",
    "        df_syslog_clean[df_syslog_clean[\"mentions_red_ip\"]]\n",
    "        .set_index(\"time\")\n",
    "        .groupby([pd.Grouper(freq=\"1h\"), \"severity\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    severity_timeline.plot(figsize=(14, 6), marker=\"o\")\n",
    "    plt.title(\"Red-Team Syslog Severity Over Time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Log Count\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Severity\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No 'severity' column found in Syslog dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76cdff8-4f9a-4790-aee2-1aadfce02b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winlog Red-Team events by severity \n",
    "if \"level\" in df_winlog_clean.columns:\n",
    "    winlog_severity_summary = (\n",
    "        df_winlog_clean[df_winlog_clean[\"mentions_red_ip\"]]\n",
    "        .groupby(\"level\")[\"message\"]\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Winlog Red-Team Events by Severity:\")\n",
    "    display(winlog_severity_summary)\n",
    "else:\n",
    "    print(\"No 'level' column found in Winlog dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96c0ef-2580-4e48-b421-8ff6342409ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview XML structure from Winlog \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "sample_xml = df_winlog_clean[\"xml\"].dropna().iloc[0]  # get first non-null XML\n",
    "root = ET.fromstring(sample_xml)\n",
    "\n",
    "# Pretty print structure and text\n",
    "def walk_xml_tree(elem, level=0):\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}<{elem.tag.split('}')[-1]}>\", end=\"\")\n",
    "    if elem.text and elem.text.strip():\n",
    "        print(\":\", elem.text.strip())\n",
    "    else:\n",
    "        print()\n",
    "    for child in elem:\n",
    "        walk_xml_tree(child, level + 1)\n",
    "\n",
    "walk_xml_tree(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5368c28-3417-437c-8d8e-26dd0b9cf450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse severity level from Winlog XML \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_winlog_level(xml_str):\n",
    "    try:\n",
    "        root = ET.fromstring(xml_str)\n",
    "        level = root.findtext(\".//{*}System/{*}Level\")  # catch namespace\n",
    "        return int(level) if level is not None else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Create 'level' column from XML\n",
    "if \"xml\" in df_winlog_clean.columns:\n",
    "    df_winlog_clean[\"level\"] = df_winlog_clean[\"xml\"].apply(extract_winlog_level)\n",
    "    print(\"Extracted Winlog severity levels from XML (refined parser)\")\n",
    "    display(df_winlog_clean[[\"time\", \"level\", \"mentions_red_ip\"]].dropna().head())\n",
    "else:\n",
    "    print(\"No 'xml' column found in Winlog dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38172f72-5603-4e9c-995e-366810b0e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric severity level from Winlog XML \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_numeric_level(xml_str):\n",
    "    try:\n",
    "        root = ET.fromstring(xml_str)\n",
    "        # Use namespace-agnostic tag access\n",
    "        for elem in root.iter():\n",
    "            if elem.tag.endswith(\"Level\") and elem.text and elem.text.strip().isdigit():\n",
    "                return int(elem.text.strip())\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "df_winlog_clean[\"level\"] = df_winlog_clean[\"xml\"].apply(extract_numeric_level)\n",
    "\n",
    "print(\"Extracted numeric Winlog severity levels from XML\")\n",
    "display(df_winlog_clean[[\"time\", \"level\", \"mentions_red_ip\"]].dropna().head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d7217-ecec-41d1-ac25-597b92f2677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of known Red-Team hostnames from the topology image\n",
    "red_hostnames = {\"red-team1\", \"red-team2\", \"red-team3\", \"red-team4\", \"red-team5\", \"red-team6\", \"red-team7\"}\n",
    "\n",
    "# Extract <Computer> tag from XML\n",
    "def extract_winlog_computer(xml_str):\n",
    "    try:\n",
    "        root = ET.fromstring(xml_str)\n",
    "        computer = root.findtext(\".//{*}System/{*}Computer\")\n",
    "        return computer.lower() if computer else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply and tag Red-Team hosts\n",
    "df_winlog_clean[\"computer\"] = df_winlog_clean[\"xml\"].apply(extract_winlog_computer)\n",
    "df_winlog_clean[\"red_host\"] = df_winlog_clean[\"computer\"].isin(red_hostnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96562a-191a-4145-ba00-0d3399359e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Red-Team Activity Across Datasets \n",
    "\n",
    "# IPFIX: select key columns for red-involved flows\n",
    "ipfix_red = (\n",
    "    df_ipfix_clean[df_ipfix_clean[\"involves_red\"]]\n",
    "    .loc[:, [\"time\", \"sourceIPv4Address\", \"destinationIPv4Address\"]]\n",
    "    .assign(source=\"IPFIX\", message=\"Flow involving Red-Team IP\")\n",
    ")\n",
    "\n",
    "# Syslog: select tagged messages\n",
    "syslog_red = (\n",
    "    df_syslog_clean[df_syslog_clean[\"mentions_red_ip\"]]\n",
    "    .loc[:, [\"time\", \"message\"]]\n",
    "    .assign(source=\"Syslog\", sourceIPv4Address=None, destinationIPv4Address=None)\n",
    ")\n",
    "\n",
    "winlog_red = (\n",
    "    df_winlog_clean[(df_winlog_clean[\"mentions_red_ip\"]) | (df_winlog_clean[\"red_host\"])]\n",
    "    .loc[:, [\"time\", \"xml\"]]\n",
    "    .rename(columns={\"xml\": \"message\"})\n",
    "    .assign(source=\"Winlog\", sourceIPv4Address=None, destinationIPv4Address=None)\n",
    ")\n",
    "\n",
    "\n",
    "# Combine all red-team activity into one timeline\n",
    "redteam_events = pd.concat([ipfix_red, syslog_red, winlog_red], ignore_index=True).sort_values(\"time\")\n",
    "\n",
    "print(\"📦 Aggregated Red-Team Event Rows:\", len(redteam_events))\n",
    "display(redteam_events.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8088a91-58a5-4176-ac0f-1fffc78891c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Red-Team Activity Timeline \n",
    "activity_timeline = (\n",
    "    redteam_events.set_index(\"time\")\n",
    "    .groupby([pd.Grouper(freq=\"1h\"), \"source\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "activity_timeline.plot(figsize=(14, 6), marker=\"o\")\n",
    "plt.title(\"Red-Team Activity Timeline by Source\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Event Count\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Log Source\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a005c-4642-4066-ab4d-9a7a09c8ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Non-Red-Team Activity Timeline \n",
    "\n",
    "# Filter non-redteam rows in each dataset\n",
    "ipfix_nonred = (\n",
    "    df_ipfix_clean[~df_ipfix_clean[\"involves_red\"]]\n",
    "    .loc[:, [\"time\", \"sourceIPv4Address\", \"destinationIPv4Address\"]]\n",
    "    .assign(source=\"IPFIX\", message=\"Flow not involving Red-Team IP\")\n",
    ")\n",
    "\n",
    "syslog_nonred = (\n",
    "    df_syslog_clean[~df_syslog_clean[\"mentions_red_ip\"]]\n",
    "    .loc[:, [\"time\", \"message\"]]\n",
    "    .assign(source=\"Syslog\", sourceIPv4Address=None, destinationIPv4Address=None)\n",
    ")\n",
    "\n",
    "winlog_nonred = (\n",
    "    df_winlog_clean[~df_winlog_clean[\"mentions_red_ip\"]]\n",
    "    .loc[:, [\"time\", \"xml\"]]\n",
    "    .rename(columns={\"xml\": \"message\"})\n",
    "    .assign(source=\"Winlog\", sourceIPv4Address=None, destinationIPv4Address=None)\n",
    ")\n",
    "\n",
    "# Combine\n",
    "nonred_events = pd.concat([ipfix_nonred, syslog_nonred, winlog_nonred], ignore_index=True).sort_values(\"time\")\n",
    "\n",
    "# Plot\n",
    "nonred_timeline = (\n",
    "    nonred_events.set_index(\"time\")\n",
    "    .groupby([pd.Grouper(freq=\"1h\"), \"source\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "nonred_timeline.plot(figsize=(14, 6), marker=\"o\")\n",
    "plt.title(\"Non-Red-Team Activity Timeline by Source\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Event Count\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Log Source\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448989b-2656-4cfb-b463-5e4a3a6eb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side Comparison of Activity Timelines \n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6), sharey=True)\n",
    "\n",
    "activity_timeline.plot(ax=axes[0], marker=\"o\", title=\"Red-Team Activity\")\n",
    "axes[0].set_xlabel(\"Time\")\n",
    "axes[0].set_ylabel(\"Event Count\")\n",
    "axes[0].grid(True)\n",
    "axes[0].legend(title=\"Source\")\n",
    "\n",
    "nonred_timeline.plot(ax=axes[1], marker=\"o\", title=\"Non-Red-Team Activity\")\n",
    "axes[1].set_xlabel(\"Time\")\n",
    "axes[1].grid(True)\n",
    "axes[1].legend(title=\"Source\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d3668-e516-4846-9684-02854635a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay Red-Team and Non-Red-Team Activity Timeline \n",
    "combined = (\n",
    "    pd.concat([\n",
    "        redteam_events.assign(team=\"Red-Team\"),\n",
    "        nonred_events.assign(team=\"Non-Red-Team\")\n",
    "    ])\n",
    "    .set_index(\"time\")\n",
    "    .groupby([pd.Grouper(freq=\"1h\"), \"team\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "combined.plot(figsize=(14, 6), marker=\"o\")\n",
    "plt.title(\"Combined Red-Team and Non-Red-Team Activity Timeline\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Event Count\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Team\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28512a77-3685-4093-892e-ed6c4b9f0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red-Team Activity Severity Overlay \n",
    "# Only Winlog has severity levels (via 'level' column)\n",
    "if \"level\" in df_winlog_clean.columns:\n",
    "    df_red_severity = df_winlog_clean[\n",
    "        (df_winlog_clean[\"mentions_red_ip\"] | df_winlog_clean[\"red_host\"]) & df_winlog_clean[\"level\"].notna()\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    df_red_severity[\"level\"] = pd.to_numeric(df_red_severity[\"level\"], errors=\"coerce\")\n",
    "\n",
    "    if not df_red_severity.empty:\n",
    "        severity_overlay = (\n",
    "            df_red_severity.set_index(\"time\")\n",
    "            .groupby([pd.Grouper(freq=\"1h\"), \"level\"])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "\n",
    "        if not severity_overlay.empty:\n",
    "            severity_overlay.plot(figsize=(14, 6), marker=\"o\")\n",
    "            plt.title(\"Red-Team Activity Severity Timeline (Winlog)\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Event Count by Severity Level\")\n",
    "            plt.grid(True)\n",
    "            plt.legend(title=\"Severity Level\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No severity data found to plot.\")\n",
    "    else:\n",
    "        print(\"No red-team severity entries found in Winlog.\")\n",
    "else:\n",
    "    print(\"No 'level' column available in Winlog data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89497b39-74f9-4d78-b983-367ba162afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Distribution of Winlog Severity Levels \n",
    "\n",
    "# Ensure 'level' is numeric\n",
    "df_winlog_clean[\"level\"] = pd.to_numeric(df_winlog_clean[\"level\"], errors=\"coerce\")\n",
    "\n",
    "# Drop NaNs and count frequency\n",
    "severity_counts = df_winlog_clean[\"level\"].dropna().value_counts().sort_index()\n",
    "\n",
    "# Bar plot\n",
    "severity_counts.plot(kind=\"bar\", figsize=(10, 5), color=\"steelblue\")\n",
    "plt.title(\"Global Distribution of Winlog Event Severity Levels\")\n",
    "plt.xlabel(\"Severity Level\")\n",
    "plt.ylabel(\"Number of Events\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display raw counts\n",
    "print(\"Event counts per severity level:\")\n",
    "display(severity_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c09b7-ab95-4d31-8bea-e19bd50f8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay Winlog Severity on Combined Timeline \n",
    "\n",
    "# Filter Winlog entries with valid level and time\n",
    "df_severity_ts = df_winlog_clean[df_winlog_clean[\"level\"].notna()].copy()\n",
    "df_severity_ts[\"level\"] = pd.to_numeric(df_severity_ts[\"level\"], errors=\"coerce\")\n",
    "\n",
    "# Aggregate: severity level count per hour\n",
    "severity_by_hour = (\n",
    "    df_severity_ts.set_index(\"time\")\n",
    "    .groupby([pd.Grouper(freq=\"1h\"), \"level\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot combined red/non-red activity\n",
    "combined.plot(ax=ax1, marker=\"o\", linewidth=2)\n",
    "ax1.set_ylabel(\"Event Count (Red vs. Non-Red)\")\n",
    "ax1.set_title(\"Combined Activity Timeline with Winlog Severity Overlay\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Overlay severity bars (stacked)\n",
    "severity_by_hour.plot(\n",
    "    ax=ax2,\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    alpha=0.3,\n",
    "    width=1.0,\n",
    "    legend=False,\n",
    "    color=plt.cm.Reds(np.linspace(0.4, 1, severity_by_hour.shape[1]))\n",
    ")\n",
    "ax2.set_ylabel(\"Winlog Events by Severity Level\")\n",
    "ax2.set_ylim(top=max(severity_by_hour.sum(axis=1).max() * 1.2, 10))  # give some headroom\n",
    "\n",
    "# Legend fix\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(handles + handles2, labels + labels2, title=\"Legend\", loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82e1a7-0788-4118-9a79-3b641c02592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Overlay: Red-Team vs Non-Red + Winlog Severity \n",
    "\n",
    "# Recalculate severity overlay for Winlog \n",
    "df_severity_ts = df_winlog_clean[df_winlog_clean[\"level\"].notna()].copy()\n",
    "df_severity_ts[\"level\"] = pd.to_numeric(df_severity_ts[\"level\"], errors=\"coerce\")\n",
    "\n",
    "severity_by_hour = (\n",
    "    df_severity_ts.set_index(\"time\")\n",
    "    .groupby([pd.Grouper(freq=\"1h\"), \"level\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Prepare plot\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot Red-Team vs Non-Red-Team activity as lines\n",
    "combined.plot(ax=ax1, marker='o', linewidth=2)\n",
    "ax1.set_ylabel(\"Event Count (Red / Non-Red)\")\n",
    "ax1.set_title(\"Red-Team vs Non-Red-Team Activity with Winlog Severity Overlay\")\n",
    "ax1.grid(True)\n",
    "ax1.set_xlabel(\"Time\")\n",
    "\n",
    "# Create second y-axis for severity levels\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot severity as stacked area plot (soft color, transparent)\n",
    "severity_by_hour.plot.area(\n",
    "    ax=ax2,\n",
    "    stacked=True,\n",
    "    alpha=0.3,\n",
    "    linewidth=0,\n",
    "    cmap=\"OrRd\"\n",
    ")\n",
    "\n",
    "ax2.set_ylabel(\"Winlog Event Count by Severity Level\")\n",
    "\n",
    "# Custom legend with separation\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Position legends clearly\n",
    "first_legend = ax1.legend(h1, l1, title=\"Activity Type\", loc=\"upper left\")\n",
    "second_legend = ax2.legend(h2, l2, title=\"Severity Level\", loc=\"upper right\")\n",
    "ax1.add_artist(first_legend)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46070160-5893-4910-866e-7a768ff90c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique severity levels in Winlog:\")\n",
    "print(df_winlog_clean[\"level\"].dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bc7ca-56d9-4643-a1cd-8ac4048eee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map severity levels to human-friendly labels\n",
    "severity_labels = {\n",
    "    0: \"LogAlways\",\n",
    "    1: \"Critical\",\n",
    "    2: \"Error\",\n",
    "    3: \"Warning\",\n",
    "    4: \"Information\"\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "severity_by_hour.columns = severity_by_hour.columns.map(severity_labels)\n",
    "\n",
    "# Plot with better colormap and legend title\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "combined.plot(ax=ax, marker=\"o\", linewidth=2, legend=True)\n",
    "severity_by_hour.plot.area(ax=ax, alpha=0.3, cmap=\"tab10\")\n",
    "\n",
    "plt.title(\"Combined Activity with Severity Overlay (Winlog)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Event Count\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Log Source & Severity\", loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d17347-e10a-4c7c-aaa7-dbe5ecf4e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numeric column if not already\n",
    "df_winlog_clean[\"level\"] = pd.to_numeric(df_winlog_clean[\"level\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with level 0 (LogAlways) and NaN\n",
    "df_severity = df_winlog_clean[df_winlog_clean[\"level\"].notna() & (df_winlog_clean[\"level\"] != 0)].copy()\n",
    "\n",
    "# Add hourly column\n",
    "df_severity[\"hour\"] = df_severity[\"time\"].dt.floor(\"h\")\n",
    "\n",
    "# Find most common severity level per hour (excluding level 0)\n",
    "top_severity_per_hour = (\n",
    "    df_severity.groupby(\"hour\")[\"level\"]\n",
    "    .agg(lambda x: x.value_counts().idxmax())\n",
    "    .rename(\"most_frequent_level\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Map level to labels\n",
    "severity_labels = {\n",
    "    1: \"Critical\",\n",
    "    2: \"Error\",\n",
    "    3: \"Warning\",\n",
    "    4: \"Information\"\n",
    "}\n",
    "top_severity_per_hour[\"label\"] = top_severity_per_hour[\"most_frequent_level\"].map(severity_labels)\n",
    "\n",
    "print(\"Most frequent severity per hour (excluding LogAlways):\")\n",
    "display(top_severity_per_hour.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ada45-e746-4e50-b7f9-fe9d40833f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the combined timeline\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "combined.plot(ax=ax, marker=\"o\", linewidth=2)\n",
    "\n",
    "# Plot top severity level as scatter points\n",
    "colors = {\n",
    "    \"Critical\": \"red\",\n",
    "    \"Error\": \"orange\",\n",
    "    \"Warning\": \"gold\",\n",
    "    \"Information\": \"blue\",\n",
    "    \"LogAlways\": \"gray\"\n",
    "}\n",
    "\n",
    "for level_label, color in colors.items():\n",
    "    points = top_severity_per_hour[top_severity_per_hour[\"label\"] == level_label]\n",
    "    ax.scatter(points[\"hour\"], [0]*len(points), label=level_label, color=color, s=40, marker=\"s\")\n",
    "\n",
    "plt.title(\"Combined Activity with Dominant Severity per Hour\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Event Count\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Team / Severity\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
